{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 5-9 Markov Decision Processes\n",
    "* The Markov Property\n",
    "* The Markov Decision Process\n",
    "* Partially Observable MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"interact-js-shim\">\n",
       "    <script charset=\"utf-8\">\r\n",
       "(function (IPython, $, _, MathJax) {\r\n",
       "    $.event.special.destroyed = {\r\n",
       "\tremove: function(o) {\r\n",
       "\t    if (o.handler) {\r\n",
       "\t\to.handler.apply(this, arguments)\r\n",
       "\t    }\r\n",
       "\t}\r\n",
       "    }\r\n",
       "\r\n",
       "    var OutputArea = IPython.version >= \"4.0.0\" ? require(\"notebook/js/outputarea\").OutputArea : IPython.OutputArea;\r\n",
       "\r\n",
       "    var redrawValue = function (container, type, val) {\r\n",
       "\tvar selector = $(\"<div/>\");\r\n",
       "\tvar oa = new OutputArea(_.extend(selector, {\r\n",
       "\t    selector: selector,\r\n",
       "\t    prompt_area: true,\r\n",
       "\t    events: IPython.events,\r\n",
       "\t    keyboard_manager: IPython.keyboard_manager\r\n",
       "\t})); // Hack to work with IPython 2.1.0\r\n",
       "\r\n",
       "\tswitch (type) {\r\n",
       "\tcase \"image/png\":\r\n",
       "            var _src = 'data:' + type + ';base64,' + val;\r\n",
       "\t    $(container).find(\"img\").attr('src', _src);\r\n",
       "\t    break;\r\n",
       "\tcase \"text/latex\":\r\n",
       "\t\tif (MathJax){\r\n",
       "\t\t\tvar math = MathJax.Hub.getAllJax(container)[0];\r\n",
       "\t\t\tMathJax.Hub.Queue([\"Text\", math, val.replace(/^\\${1,2}|\\${1,2}$/g, '')]);\r\n",
       "\t\t\tbreak;\r\n",
       "\t\t}\r\n",
       "\tdefault:\r\n",
       "\t    var toinsert = OutputArea.append_map[type].apply(\r\n",
       "\t\toa, [val, {}, selector]\r\n",
       "\t    );\r\n",
       "\t    $(container).empty().append(toinsert.contents());\r\n",
       "\t    selector.remove();\r\n",
       "\t}\r\n",
       "    }\r\n",
       "\r\n",
       "\r\n",
       "    $(document).ready(function() {\r\n",
       "\tfunction initComm(evt, data) {\r\n",
       "\t    var comm_manager = data.kernel.comm_manager;\r\n",
       "        //_.extend(comm_manager.targets, require(\"widgets/js/widget\"))\r\n",
       "\t    comm_manager.register_target(\"Signal\", function (comm) {\r\n",
       "            comm.on_msg(function (msg) {\r\n",
       "                var val = msg.content.data.value;\r\n",
       "                $(\".signal-\" + comm.comm_id).each(function() {\r\n",
       "                var type = $(this).data(\"type\");\r\n",
       "                if (typeof(val[type]) !== \"undefined\" && val[type] !== null) {\r\n",
       "                    redrawValue(this, type, val[type], type);\r\n",
       "                }\r\n",
       "                });\r\n",
       "                delete val;\r\n",
       "                delete msg.content.data.value;\r\n",
       "            });\r\n",
       "\t    });\r\n",
       "\r\n",
       "\t    // coordingate with Comm and redraw Signals\r\n",
       "\t    // XXX: Test using Reactive here to improve performance\r\n",
       "\t    $([IPython.events]).on(\r\n",
       "\t\t'output_appended.OutputArea', function (event, type, value, md, toinsert) {\r\n",
       "\t\t    if (md && md.reactive) {\r\n",
       "                // console.log(md.comm_id);\r\n",
       "                toinsert.addClass(\"signal-\" + md.comm_id);\r\n",
       "                toinsert.data(\"type\", type);\r\n",
       "                // Signal back indicating the mimetype required\r\n",
       "                var comm_manager = IPython.notebook.kernel.comm_manager;\r\n",
       "                var comm = comm_manager.comms[md.comm_id];\r\n",
       "                comm.then(function (c) {\r\n",
       "                    c.send({action: \"subscribe_mime\",\r\n",
       "                       mime: type});\r\n",
       "                    toinsert.bind(\"destroyed\", function() {\r\n",
       "                        c.send({action: \"unsubscribe_mime\",\r\n",
       "                               mime: type});\r\n",
       "                    });\r\n",
       "                })\r\n",
       "\t\t    }\r\n",
       "\t    });\r\n",
       "\t}\r\n",
       "\r\n",
       "\ttry {\r\n",
       "\t    // try to initialize right away. otherwise, wait on the status_started event.\r\n",
       "\t    initComm(undefined, IPython.notebook);\r\n",
       "\t} catch (e) {\r\n",
       "\t    $([IPython.events]).on('kernel_created.Kernel kernel_created.Session', initComm);\r\n",
       "\t}\r\n",
       "    });\r\n",
       "})(IPython, jQuery, _, MathJax);\r\n",
       "</script>\n",
       "    <script>\n",
       "        window.interactLoadedFlag = true\n",
       "       $(\"#interact-js-shim\").bind(\"destroyed\", function () {\n",
       "           if (window.interactLoadedFlag) {\n",
       "               console.warn(\"JavaScript required by Interact will be removed if you remove this cell or run using Interact more than once.\")\n",
       "           }\n",
       "       })\n",
       "       $([IPython.events]).on(\"kernel_starting.Kernel kernel_restarting.Kernel\", function () { window.interactLoadedFlag = false })\n",
       "   </script>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n"
      ],
      "text/plain": [
       "Revealables.Revealable(\"###Answer F\\n1. Three is a challenge! I had to look it up. I won't give away the third, but there are a lot of possibilities. I'd be worried if their belief state wasn't something like: allergies 48%, cold 48%, <other> 2%. Make them fill in \\\"other\\\", even if they have to look it up too. It was educational.\\n2. I personally don't like over-medicating my patients (maybe this is why I'm a math teacher) so my action would be to wait a week and do nothing.\\n3. If they're still sneezing after waiting a week, my belief state would become: allergies 80%, cold 15%, <other> 5%.\\n4. They could get really insightful here. I would ask a lot of questions trying to eliminate the \\\"other\\\" hypothesis and narrow it down to cold or allergies, and then I would try to narrow it between those (is it seasonal? what seasons? do you work with small children? is your family sick?). I would try more wait time which would hopefully give a cold time to pass on, and then see if allergy meds helped. If they did, then there's the answer. If they didn't, then I would conclude that while allergies are still possible they are much less probable. At some point \\\"cold\\\" and \\\"allergies\\\" would both be so improbable I'd have to look deeper into the \\\"other\\\" option and maybe expand my state possibilities beyond three.\\n\\nYou gain a lot of insight into medicine by thinking of doctors as POMDP systems. Probably because they are.\\n\", \"Answer\", false, false)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Revealables\n",
    "include(\"files/answers.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Premise\n",
    "Much of the time, statistics are thought of as being very deterministic, for example: \n",
    "* 79.8% of Stanford students graduate in 4 years.\n",
    "  ([www.collegeresults.org](http://www.collegeresults.org), 2014)\n",
    "  \n",
    "It's very tempting to read this sort of statistic as if graduating from Stanford in four years is a randomly determined event. In fact, it's a combination of two things:\n",
    "* random chance, which does play a role\n",
    "* the actions of the student\n",
    "\n",
    "Situations like this, in which outcomes are a combination of random chance and active choices, can still be analyzed and optimized. \n",
    "\n",
    "You can't optimize the part that happens randomly, but you can still find the best actions to take: in other words, you can optimize your actions to find the best outcome in spite of the random element.\n",
    "\n",
    "One way to do this is by using a Markov Decision Process.\n",
    "\n",
    "##The Markov Property\n",
    "__Markov Decision Processes (MDPs)__ are stochastic processes that exhibit the __Markov Property__.\n",
    "\n",
    "* Recall that stochastic processes, in Unit 2, were processes that involve randomness. The examples in unit 2 were not influenced by any active choices â€“ everything was random. This is why they could be analyzed without using MDPs.\n",
    "* The Markov Property is used to refer to situations where the probabilities of different outcomes are not dependent on past states: the current state is all you need to know. This property is sometimes called \"memorylessness.\" \n",
    "\n",
    "For example, if an unmanned aircraft is trying to remain level, all it needs to know is its current state, which might include how level it currently is, and what influences (momentum, wind, gravity) are acting upon its state of level-ness. This analysis displays the Markov Property.\n",
    "\n",
    "In contrast, if an unmanned aircraft is trying to figure out how long until its battery dies, it would need to know not only how much charge it has now, but also how fast that charge has declined from a past state. Therefore, this does not display the Markov Property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem A\n",
    "For each scenario:\n",
    "* tell whether the outcome is influenced by chance only or a combination of chance and action\n",
    "* tell whether the outcome's probabilities depend only on the present or partially on the past as well\n",
    "* argue whether this scenario can or can not be analyzed with an MDP\n",
    "1. A Stanford freshman wants to graduate in 4 years.\n",
    "2. An unmanned vehicle wants to avoid collision.\n",
    "3. A gambler wants to win at roulette.\n",
    "4. A truck driver wants to get from his current location to Los Angeles. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write your answers here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "Interact.ToggleButton(1: \"input\" = false Bool , \"Answer\", false)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n"
      ],
      "text/plain": [
       "Revealables.Revealable(\"###Answer A\\nThe reasoning is more important than the answer.\\n1. Action is important, but the past is also likely to be influential. This is not a suitable MDP scenario.\\n2. Action is important, and if it knows its current state (speed, vehicles around it, etc.) the past shouldn't be vital. MDP could be used.\\n3. Debatable. The guy has to choose red or black.  But really, mostly chance. It is, however, not past-dependent. I'd say not an MDP candidate.\\n4. Action is important, and if he knows his current state (i.e., his location) he doesn't really need to know the past. MDP candidate.\\n\", \"Answer\", false, false)"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "comm_id": "7243a694-88e9-4f83-9ec4-d5ad44e0befb",
      "reactive": true
     },
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Compat.ASCIIString is deprecated, use String instead.\n",
      "  likely near C:\\Users\\Victoria Docherty\\.julia\\v0.6\\IJulia\\src\\kernel.jl:31\n",
      "WARNING: Compat.ASCIIString is deprecated, use String instead.\n",
      "  likely near C:\\Users\\Victoria Docherty\\.julia\\v0.6\\IJulia\\src\\kernel.jl:31\n",
      "WARNING: Compat.ASCIIString is deprecated, use String instead.\n",
      "  likely near C:\\Users\\Victoria Docherty\\.julia\\v0.6\\IJulia\\src\\kernel.jl:31\n",
      "WARNING: Compat.ASCIIString is deprecated, use String instead.\n",
      "  likely near C:\\Users\\Victoria Docherty\\.julia\\v0.6\\IJulia\\src\\kernel.jl:31\n"
     ]
    }
   ],
   "source": [
    "revealable(ans509A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Aspects of an MDP\n",
    "Some important aspects of a Markov Decision Process:\n",
    "* <font color=\"blue\">State</font>: a set of existing or theoretical conditions, like position, color, velocity, environment, amount of resources, etc.\n",
    "\n",
    "  One of the challenges in designing an MDP is to figure out what all the possible states are. \n",
    "\n",
    "  The current state is only one of a large set of possible states, some more desirable than others.\n",
    "\n",
    "  Even though an object only has one current state, it will probably end up in a different state at some point.<br /><br />\n",
    "\n",
    "* <font color=\"red\">Action</font>: just like there is a large set of possible states, there is also a large set of possible actions that might be taken.\n",
    "\n",
    "  The current state often influences which actions are available. For example, if you are driving a car, your options for turning left or right are often restricted by what lane you are in.\n",
    "\n",
    "\n",
    "* A __<font color=\"orange\">Probability Distribution</font>__ is used to determine the transition from the current state to the next state. \n",
    "\n",
    "  The probability of one state (flying sideways) leading to another (crashing horribly) is influenced by both action and chance. The same state (flying sideways) may also lead to other states (recovering, turning, landing safely) with different probabilities, which also depend on both actions and chance. \n",
    "\n",
    "  All of these different probabilities are called the probability distribution, which is often contained in a matrix.\n",
    "\n",
    "* <font color=\"green\">Reward</font>: calculated based on the value of the next state compared to the current state. More favorable states generate better rewards.\n",
    "\n",
    "  For example, if a plane is flying sideways, the reward for recovering would be much higher than the reward for crashing horribly.\n",
    "\n",
    "##The Markov Decision Process\n",
    "Once the states, actions, probability distribution, and rewards have been determined, the last task is to run the process. A <font color=\"purple\">time step</font> is determined, and the state is monitored at each time step.\n",
    "\n",
    "In a simulation,\n",
    "1. The initial state is chosen randomly from the set of possible states. \n",
    "2. Based on that state, an action is chosen. \n",
    "3. The next state is determined based on the probability distribution for the given state and the action chosen.\n",
    "4. A reward is granted for the next state.\n",
    "5. The entire process is repeated from step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem B\n",
    "Suppose a system has two states, $\\color{green}{A}$ (good) and $\\color{red}{B}$ (bad). There are also two actions, $\\color{purple}{x}$ and $\\color{blue}{y}$. \n",
    "\n",
    "From state $\\color{green}{A}$, $\\color{purple}{x}$ leads to $\\color{green}{A}$ (60%) or $\\color{red}{B}$ (40%); $\\color{blue}{y}$ leads to $\\color{green}{A}$ (50%) or $\\color{red}{B}$ (50%).\n",
    "\n",
    "From state $\\color{red}{B}$, $\\color{purple}{x}$ leads to $\\color{green}{A}$ (30%) or $\\color{red}{B}$(70%); $\\color{blue}{y}$ leads to $\\color{green}{A}$(80%) or $\\color{red}{B}$(20%).\n",
    "\n",
    "1. Run a simulation starting in state $\\color{green}{A}$ and repeating action $\\color{purple}{x}$ ten times. Use `rand()` to generate the probabilities. For each time step you are in state $\\color{green}{A}$, give yourself 1 point.\n",
    "2. Run the same simulation using action $\\color{blue}{y}$ ten times.\n",
    "3. Run the same simulation alternating $\\color{purple}{x}$ and $\\color{blue}{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Run simulations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "Interact.ToggleButton(4: \"input-2\" = false Bool , \"Answer\", false)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n"
      ],
      "text/plain": [
       "Revealables.Revealable(\"###Answer B\\nAnswers will vary; my states came out like this:\\n1. A A A A B A A B B B, 6 points\\n2. A B A B B A A A A B, 6 points\\n3. A A B B A B A B A A, 6 points\\n\", \"Answer\", false, false)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "comm_id": "c22aa70b-161c-4e83-87c9-8cbc5fd49bbc",
      "reactive": true
     },
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Compat.ASCIIString is deprecated, use String instead.\n",
      "  likely near C:\\Users\\Victoria Docherty\\.julia\\v0.6\\IJulia\\src\\kernel.jl:31\n",
      "WARNING: Compat.ASCIIString is deprecated, use String instead.\n",
      "  likely near C:\\Users\\Victoria Docherty\\.julia\\v0.6\\IJulia\\src\\kernel.jl:31\n"
     ]
    }
   ],
   "source": [
    "revealable(ans509B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem C\n",
    "Write two functions in Julia, `actionx(k)` and `actiony(k)`, that take an input `k` (the state $A$ or $B$) and return an output `A` or `B` based on the probabilities from the preceding problem:\n",
    "\n",
    "From state $A$, $x$ leads to $A$(60%) or $B$(40%); $y$ leads to $A$(50%) or $B$(50%). From state $B$, $x$ leads to $A$(30%) or $B$(70%); y leads to $A$(80%) or $B$(20%).\n",
    "\n",
    "Use your program in a simulation to find the total reward value of repeating $x$ 100 times or repeating $y$ 100 times. Use multiple simulations. Which is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Write your functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "repeater (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "Interact.ToggleButton(7: \"input-3\" = false Bool , \"Answer\", false)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n"
      ],
      "text/plain": [
       "Revealables.Revealable(\"###Answer C\\nFor `x`, I got a bunch of wild swings, probably a higher overall average but ranging from mid 40s to low 60s. For `y`, I got a lot of mid-50s, fairly consistently.\\n\\nIn my code, I chose to use `\\\"A\\\"` and `\\\"B\\\"` literally, rather than a proxy like `1` and `0`. The number proxy would probably be easier.\\n<code>\\nfunction actionx(x)\\n    if x == \\\"A\\\"\\n        if rand()<.6\\n            return(\\\"A\\\")\\n        else\\n            return(\\\"B\\\")\\n        end\\n    elseif x == \\\"B\\\"\\n        if rand()<.5\\n            return(\\\"A\\\")\\n        else\\n            return(\\\"B\\\")\\n        end\\n    end\\n end\\n\\nfunction actiony(x)\\n    if x == \\\"A\\\"\\n        if rand()<.3\\n            return(\\\"A\\\")\\n        else\\n            return(\\\"B\\\")\\n        end\\n    elseif x == \\\"B\\\"\\n        if rand()<.8\\n            return(\\\"A\\\")\\n        else\\n            return(\\\"B\\\")\\n        end\\n    end\\n end\\n\\nfunction repeater(x)\\n    value = 0\\n    for n in 1:100\\n        x = actionx(x)\\n        if x == \\\"A\\\"\\n            value = value + 1\\n        end\\n    end\\n    return(value)\\nend\\n</code>\\n\\n\", \"Answer\", false, false)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "comm_id": "64d2aae6-1223-4dda-834a-0531516dfa75",
      "reactive": true
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revealable(ans509C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Probability Distribution Matrices\n",
    "In the previous problem, the probabilities for action $x$ were as follows: From state $A$, $x$ leads to $A$(60%) or $B$(40%); from state $B$, $x$ leads to $A$(30%) or $B$(70%).\n",
    "\n",
    "This information can be summarized more briefly in a matrix: \n",
    "$$\\begin{array}{cc}\n",
    " \\\\\n",
    "\\begin{array}{cc}\n",
    "\\qquad \\color{green}{A} & \\color{green}{B}\n",
    "\\end{array} \\\\\n",
    "\\begin{array}{c}\n",
    "\\color{purple}{A} \\\\\n",
    "\\color{purple}{B}\n",
    "\\end{array}\n",
    "\\left[\\begin{array}{cc}\n",
    ".6 & .4 \\\\\n",
    ".3 & .7\n",
    "\\end{array}\\right]\n",
    "\\end{array}$$\n",
    "\n",
    "* <font color=\"purple\">current state</font>\n",
    "* <font color=\"green\">next state</font>\n",
    "\n",
    "Matrices enable neat probability summaries even for very large sets of possible events or \"event spaces.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem D\n",
    "Given this probability distribution for action $k$:\n",
    "$$\\begin{array}{cc}\n",
    "~ \\\\\n",
    "\\begin{array}{ccc}\n",
    "\\qquad \\color{green}{A} & \\color{green}{B} & \\color{green}{C}\n",
    "\\end{array} \\\\\n",
    "\\begin{array}{c}\n",
    "\\color{purple}{A} \\\\\n",
    "\\color{purple}{B} \\\\\n",
    "\\color{purple}{C}\n",
    "\\end{array}\\\n",
    "\\left[\\begin{array}{ccc}\n",
    ".1&.2&.7 \\\\\n",
    ".8&0&.2 \\\\\n",
    "0&.4&.6\n",
    "\\end{array}\\right]\n",
    "\\end{array}$$\n",
    "\n",
    "Tell the probability that action $k$ will move from state:\n",
    "1. $A$ to $B$\n",
    "2. $A$ to $C$\n",
    "3. $C$ to $A$\n",
    "4. $C$ to $C$\n",
    "5. $B$ to $C$\n",
    "6. $B$ to $A$\n",
    "7. Why do the rows have to add up to 1, but the columns not?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write answers here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "Interact.ToggleButton(10: \"input-4\" = false Bool , \"Answer\", false)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n"
      ],
      "text/plain": [
       "Revealables.Revealable(\"###Answer D\\n.2, .7, 0, .6, .2, 8\\n\\nGiven a current state (rows), one of the other states must result. But given a new state (columns), it could have come from any of the previous states in any combination.\\n\", \"Answer\", false, false)"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "comm_id": "d16d7315-f4c4-495b-b1ec-a54b76b3fdd3",
      "reactive": true
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revealable(ans509D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Intelligent Systems\n",
    "In Practice Problem B, the action taken did not depend on the state: $x$ was chosen or $y$ was chosen regardless of the current state.\n",
    "\n",
    "However, given that $A$ was the desired state, the most intelligent course of action would be the one most likely to return $A$, which was a different depending on whether the current state was $A$ or $B$.\n",
    "\n",
    "A system that monitors its own state and chooses the best action based on its state is said to display __intelligence__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem E\n",
    "Here are the matrices for actions x and y again:\n",
    "\n",
    "action x: $\\begin{array}{cc}\n",
    " \\\\\n",
    "\\begin{array}{cc}\n",
    "\\qquad \\color{green}{A} & \\color{green}{B}\n",
    "\\end{array} \\\\\n",
    "\\begin{array}{c}\n",
    "\\color{purple}{A} \\\\\n",
    "\\color{purple}{B}\n",
    "\\end{array}\n",
    "\\left[\\begin{array}{cc}\n",
    ".6 & .4 \\\\\n",
    ".3 & .7 \n",
    "\\end{array}\\right]\n",
    "\\end{array}$, action y: \n",
    "$\\begin{array}{cc}\n",
    " \\\\\n",
    "\\begin{array}{cc}\n",
    "\\qquad \\color{green}{A} & \\color{green}{B}\n",
    "\\end{array} \\\\\n",
    "\\begin{array}{c}\n",
    "\\color{purple}{A} \\\\\n",
    "\\color{purple}{B}\n",
    "\\end{array}\\\n",
    "\\left[\\begin{array}{cc}\n",
    ".5&.5 \\\\\n",
    ".8&.2\n",
    "\\end{array}\\right]\n",
    "\\end{array}$\n",
    "\n",
    "1. In state $A$, what action is the best?\n",
    "2. In state $B$, what action is the best?\n",
    "3. Write a program in Julia that chooses the best action based on the current state. Run several 100-time-step simulations using your intelligent program."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer #1 and #2 here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Program here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "Interact.ToggleButton(13: \"input-5\" = false Bool , \"Answer\", false)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n"
      ],
      "text/plain": [
       "Revealables.Revealable(\"###Answer E\\n1. action x\\n2. action y\\n3. I got consistent high 60s/low 70s.\\n\\nThis works, but it would be nicer to combine all the actions into one function so we don't repeat so many lines of code.\\n<code>\\nfunction actionx(x)\\n    if x == \\\"A\\\"\\n        if rand()<.6\\n            return(\\\"A\\\")\\n        else\\n            return(\\\"B\\\")\\n        end\\n    elseif x == \\\"B\\\"\\n        if rand()<.5\\n            return(\\\"A\\\")\\n        else\\n            return(\\\"B\\\")\\n        end\\n    end\\n end\\n\\nfunction actiony(x)\\n    if x == \\\"A\\\"\\n        if rand()<.3\\n            return(\\\"A\\\")\\n        else\\n            return(\\\"B\\\")\\n        end\\n    elseif x == \\\"B\\\"\\n        if rand()<.8\\n            return(\\\"A\\\")\\n        else\\n            return(\\\"B\\\")\\n        end\\n    end\\n end\\n\\nfunction intelligent(x)\\n    value = 0\\n    for n in 1:100\\n        if x == \\\"A\\\"\\n            x = actionx(x)\\n            value = value + 1\\n        elseif x == \\\"B\\\"\\n            x = actiony(x)\\n        end\\n    end\\n    println(value)\\nend\\n</code>\\n\", \"Answer\", false, false)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "comm_id": "11f91de8-e5c1-441b-bc8f-93fd18af805c",
      "reactive": true
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revealable(ans509E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "Interact.ToggleButton(16: \"input-6\" = false Bool , \"Here's a slightly more elegant version:\", false)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n"
      ],
      "text/plain": [
       "Revealables.Revealable(\"###Answer E (again)\\nHere's another version to look at. Programmers generally don't like to have code that is copy-and-pasted over; repetition is what loops are for!\\n\\nI defined the states and actions using integers that are the row and column numbers for my matrix.\\n\\nFor the matrix, I got away with using a 2D array because I only had two states and two actions to worry about. The variable `probmatrix` gives probabilities of moving to State A. If you randomly generate a number above that probability, you automatically go to State B because it's the only option left. If there were more states than just two, you'd have to figure out which of the remaining states you go to next, so you'd need a different implementation.\\n\\nI'm sure there's a better way to do this!\\n<code>\\nA = 1  # matrix row number\\nB = 2\\nx = 1  # matrix column number\\ny = 2\\nprobmatrix = [.6 .5; .3 .8]  # the probability of moving to state A for each state/action combo\\n\\nfunction transition(state, action)\\n    if rand() < probmatrix[state, action]\\n        return(A)\\n    else\\n        return(B)\\n    end\\nend\\n\\nfunction reward(n)  # n: the number of trials; I usually run 100\\n    value = 0\\n    state = rand(A:B)\\n    for i in 1:n\\n        if state == A\\n            action = x  # x is always the best action to choose from state A\\n        elseif state == B\\n            action = y  # y is always the best action to choose from state B\\n        end\\n        state = transition(state, action)\\n        if state == A\\n            value = value + 1\\n        end\\n    end\\n    println(value)\\nend\\n</code>\\n\", \"Here's a slightly more elegant version:\", false, false)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "comm_id": "f27433df-3881-4f64-88c9-7870a5268526",
      "reactive": true
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revealable(ans509E2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Applications of MDPs\n",
    "Most systems, of course, are far more complex than the two-state, two-action example given here. As you can imagine, running simulations for every possible action in every possible state requires some powerful computing. \n",
    "\n",
    "MDPs are used extensively in robotics, automated systems, manufacturing, and economics, among other fields.\n",
    "\n",
    "##POMDPs\n",
    "A variation on the traditional MDP is a __Partially Observable Markov Decision Process__ (POMDP, pronounced \"Pom D.P.\"}. In these scenarios, the system does not know exactly what state it is currently in, and therefore has to guess.\n",
    "\n",
    "This is like the difference between thinking, \n",
    "* \"I'm going in the right direction\"\n",
    "  and\n",
    "* \"I *think* I'm going in the right direction.\"\n",
    "\n",
    "POMDPs have the same elements as a traditional MDP, plus two more.\n",
    "The first is the belief state, which is the state the system believes it is in. The belief state is a probability distribution.\n",
    "For example, \n",
    "\"I think Iâ€™m going in the right direction\"\n",
    "might really mean:\n",
    "* 80% chance this is the right direction\n",
    "* 15% mostly-right direction\n",
    "* 5% completely wrong direction \n",
    "\n",
    "The second additional element is a set of __observations__. After the system takes an action based on its belief state, it observes what happens next and updates its belief state accordingly.\n",
    "\n",
    "For example, if you took a right turn and didnâ€™t see the freeway you expected, you would then change your probability distribution for whether you were going in the right direction.\n",
    "\n",
    "Otherwise, a POMDP works much like a traditional MDP:\n",
    "* An action is chosen based on the belief state *(not the current state, as in a traditional MDP)*\n",
    "* The next state is reached and the reward granted\n",
    "* An observation is made and the belief state updated *(This step is absent in a traditional MDP)*\n",
    "\n",
    "\n",
    "Then the next action is chosen based on the belief state and the process repeats.\n",
    "\n",
    "##Applications of POMDPs\n",
    "POMDPs are used in robotics, automated systems, medicine, machine maintenance, biology,  networks, linguistics, and many other fields.\n",
    "\n",
    "There are also many variations on POMDP frameworks depending on the application. Some rely on Monte Carlo-style simulations. Others are used in machine learning applications (in which computers learn from trial and error rather than relying only on programmersâ€™ instructions). \n",
    "Because POMDPs are widely applicable and also fairly new on the scene, fresh variations are in constant development.\n",
    "\n",
    "[More information is available online.](http://www.cc.gatech.edu/~mstilman/class/RIP13/materials/POMDPapplications.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem F\n",
    "Imagine that you are a doctor trying to help a sneezy patient.\n",
    "1. Name three possible states for \"Why this patient sneezes\" and assign a probability to each state. (This is your belief state.)\n",
    "2. Choose an action based on your belief state.\n",
    "3. Suppose the patient is still sneezing after your action. Update your belief state.\n",
    "4. Write a paragraph explaining your reasoning in 1 through 3, and what other steps you might take (including questions to ask or other actions to try) to refine your belief state."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "Interact.ToggleButton(19: \"input-7\" = false Bool , \"Answer\", false)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n"
      ],
      "text/plain": [
       "Revealables.Revealable(\"###Answer F\\n1. Three is a challenge! I had to look it up. I won't give away the third, but there are a lot of possibilities. I'd be worried if their belief state wasn't something like: allergies 48%, cold 48%, <other> 2%. Make them fill in \\\"other\\\", even if they have to look it up too. It was educational.\\n2. I personally don't like over-medicating my patients (maybe this is why I'm a math teacher) so my action would be to wait a week and do nothing.\\n3. If they're still sneezing after waiting a week, my belief state would become: allergies 80%, cold 15%, <other> 5%.\\n4. They could get really insightful here. I would ask a lot of questions trying to eliminate the \\\"other\\\" hypothesis and narrow it down to cold or allergies, and then I would try to narrow it between those (is it seasonal? what seasons? do you work with small children? is your family sick?). I would try more wait time which would hopefully give a cold time to pass on, and then see if allergy meds helped. If they did, then there's the answer. If they didn't, then I would conclude that while allergies are still possible they are much less probable. At some point \\\"cold\\\" and \\\"allergies\\\" would both be so improbable I'd have to look deeper into the \\\"other\\\" option and maybe expand my state possibilities beyond three.\\n\\nYou gain a lot of insight into medicine by thinking of doctors as POMDP systems. Probably because they are.\\n\", \"Answer\", false, false)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "comm_id": "6c788cc8-009c-4358-9045-f8dbe9d0a85e",
      "reactive": true
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "revealable(ans509F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
