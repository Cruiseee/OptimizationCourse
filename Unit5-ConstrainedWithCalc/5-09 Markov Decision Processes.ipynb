{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 5-9 Markov Decision Processes\n",
    "* The Markov Property\n",
    "* The Markov Decision Process\n",
    "* Partially Observable MDPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Revealables\n",
    "include(\"files/answers.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Premise\n",
    "Much of the time, statistics are thought of as being very deterministic, for example: \n",
    "* 79.8% of Stanford students graduate in 4 years.\n",
    "  ([www.collegeresults.org](http://www.collegeresults.org), 2014)\n",
    "  \n",
    "It's very tempting to read this sort of statistic as if graduating from Stanford in four years is a randomly determined event. In fact, it's a combination of two things:\n",
    "* random chance, which does play a role\n",
    "* the actions of the student\n",
    "\n",
    "Situations like this, in which outcomes are a combination of random chance and active choices, can still be analyzed and optimized. \n",
    "\n",
    "You can't optimize the part that happens randomly, but you can still find the best actions to take: in other words, you can optimize your actions to find the best outcome in spite of the random element.\n",
    "\n",
    "One way to do this is by using a Markov Decision Process.\n",
    "\n",
    "##The Markov Property\n",
    "__Markov Decision Processes (MDPs)__ are stochastic processes that exhibit the __Markov Property__.\n",
    "\n",
    "* Recall that stochastic processes, in Unit 2, were processes that involve randomness. The examples in unit 2 were not influenced by any active choices â€“ everything was random. This is why they could be analyzed without using MDPs.\n",
    "* The Markov Property is used to refer to situations where the probabilities of different outcomes are not dependent on past states: the current state is all you need to know. This property is sometimes called \"memorylessness.\" \n",
    "\n",
    "For example, if an unmanned aircraft is trying to remain level, all it needs to know is its current state, which might include how level it currently is, and what influences (momentum, wind, gravity) are acting upon its state of level-ness. This analysis displays the Markov Property.\n",
    "\n",
    "In contrast, if an unmanned aircraft is trying to figure out how long until its battery dies, it would need to know not only how much charge it has now, but also how fast that charge has declined from a past state. Therefore, this does not display the Markov Property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem A\n",
    "For each scenario:\n",
    "* tell whether the outcome is influenced by chance only or a combination of chance and action\n",
    "* tell whether the outcome's probabilities depend only on the present or partially on the past as well\n",
    "* argue whether this scenario can or can not be analyzed with an MDP\n",
    "1. A Stanford freshman wants to graduate in 4 years.\n",
    "2. An unmanned vehicle wants to avoid collision.\n",
    "3. A gambler wants to win at roulette.\n",
    "4. A truck driver wants to get from his current location to Los Angeles. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write your answers here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "revealable(ans509A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Aspects of an MDP\n",
    "Some important aspects of a Markov Decision Process:\n",
    "* <font color=\"blue\">State</font>: a set of existing or theoretical conditions, like position, color, velocity, environment, amount of resources, etc.\n",
    "\n",
    "  One of the challenges in designing an MDP is to figure out what all the possible states are. \n",
    "\n",
    "  The current state is only one of a large set of possible states, some more desirable than others.\n",
    "\n",
    "  Even though an object only has one current state, it will probably end up in a different state at some point.<br /><br />\n",
    "\n",
    "* <font color=\"red\">Action</font>: just like there is a large set of possible states, there is also a large set of possible actions that might be taken.\n",
    "\n",
    "  The current state often influences which actions are available. For example, if you are driving a car, your options for turning left or right are often restricted by what lane you are in.\n",
    "\n",
    "\n",
    "* A __<font color=\"orange\">Probability Distribution</font>__ is used to determine the transition from the current state to the next state. \n",
    "\n",
    "  The probability of one state (flying sideways) leading to another (crashing horribly) is influenced by both action and chance. The same state (flying sideways) may also lead to other states (recovering, turning, landing safely) with different probabilities, which also depend on both actions and chance. \n",
    "\n",
    "  All of these different probabilities are called the probability distribution, which is often contained in a matrix.\n",
    "\n",
    "* <font color=\"green\">Reward</font>: calculated based on the value of the next state compared to the current state. More favorable states generate better rewards.\n",
    "\n",
    "  For example, if a plane is flying sideways, the reward for recovering would be much higher than the reward for crashing horribly.\n",
    "\n",
    "##The Markov Decision Process\n",
    "Once the states, actions, probability distribution, and rewards have been determined, the last task is to run the process. A <font color=\"purple\">time step</font> is determined, and the state is monitored at each time step.\n",
    "\n",
    "In a simulation,\n",
    "1. The initial state is chosen randomly from the set of possible states. \n",
    "2. Based on that state, an action is chosen. \n",
    "3. The next state is determined based on the probability distribution for the given state and the action chosen.\n",
    "4. A reward is granted for the next state.\n",
    "5. The entire process is repeated from step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem B\n",
    "Suppose a system has two states, $\\color{green}{A}$ (good) and $\\color{red}{B}$ (bad). There are also two actions, $\\color{purple}{x}$ and $\\color{blue}{y}$. \n",
    "\n",
    "From state $\\color{green}{A}$, $\\color{purple}{x}$ leads to $\\color{green}{A}$ (60%) or $\\color{red}{B}$ (40%); $\\color{blue}{y}$ leads to $\\color{green}{A}$ (50%) or $\\color{red}{B}$ (50%).\n",
    "\n",
    "From state $\\color{red}{B}$, $\\color{purple}{x}$ leads to $\\color{green}{A}$ (30%) or $\\color{red}{B}$(70%); $\\color{blue}{y}$ leads to $\\color{green}{A}$(80%) or $\\color{red}{B}$(20%).\n",
    "\n",
    "1. Run a simulation starting in state $\\color{green}{A}$ and repeating action $\\color{purple}{x}$ ten times. Use `rand()` to generate the probabilities. For each time step you are in state $\\color{green}{A}$, give yourself 1 point.\n",
    "2. Run the same simulation using action $\\color{blue}{y}$ ten times.\n",
    "3. Run the same simulation alternating $\\color{purple}{x}$ and $\\color{blue}{y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run simulations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "revealable(ans509B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem C\n",
    "Write two functions in Julia, `actionx(k)` and `actiony(k)`, that take an input `k` (the state $A$ or $B$) and return an output `A` or `B` based on the probabilities from the preceding problem:\n",
    "\n",
    "From state $A$, $x$ leads to $A$(60%) or $B$(40%); $y$ leads to $A$(50%) or $B$(50%). From state $B$, $x$ leads to $A$(30%) or $B$(70%); y leads to $A$(80%) or $B$(20%).\n",
    "\n",
    "Use your program in a simulation to find the total reward value of repeating $x$ 100 times or repeating $y$ 100 times. Use multiple simulations. Which is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your functions here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "revealable(ans509C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Probability Distribution Matrices\n",
    "In the previous problem, the probabilities for action $x$ were as follows: From state $A$, $x$ leads to $A$(60%) or $B$(40%); from state $B$, $x$ leads to $A$(30%) or $B$(70%).\n",
    "\n",
    "This information can be summarized more briefly in a matrix: \n",
    "$$\\begin{array}{cc}\n",
    " \\\\\n",
    "\\begin{array}{cc}\n",
    "\\qquad \\color{green}{A} & \\color{green}{B}\n",
    "\\end{array} \\\\\n",
    "\\begin{array}{c}\n",
    "\\color{purple}{A} \\\\\n",
    "\\color{purple}{B}\n",
    "\\end{array}\n",
    "\\left[\\begin{array}{cc}\n",
    ".6 & .4 \\\\\n",
    ".3 & .7\n",
    "\\end{array}\\right]\n",
    "\\end{array}$$\n",
    "\n",
    "* <font color=\"purple\">current state</font>\n",
    "* <font color=\"green\">next state</font>\n",
    "\n",
    "Matrices enable neat probability summaries even for very large sets of possible events or \"event spaces.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem D\n",
    "Given this probability distribution for action $k$:\n",
    "$$\\begin{array}{cc}\n",
    "~ \\\\\n",
    "\\begin{array}{ccc}\n",
    "\\qquad \\color{green}{A} & \\color{green}{B} & \\color{green}{C}\n",
    "\\end{array} \\\\\n",
    "\\begin{array}{c}\n",
    "\\color{purple}{A} \\\\\n",
    "\\color{purple}{B} \\\\\n",
    "\\color{purple}{C}\n",
    "\\end{array}\\\n",
    "\\left[\\begin{array}{ccc}\n",
    ".1&.2&.7 \\\\\n",
    ".8&0&.2 \\\\\n",
    "0&.4&.6\n",
    "\\end{array}\\right]\n",
    "\\end{array}$$\n",
    "\n",
    "Tell the probability that action $k$ will move from state:\n",
    "1. $A$ to $B$\n",
    "2. $A$ to $C$\n",
    "3. $C$ to $A$\n",
    "4. $C$ to $C$\n",
    "5. $B$ to $C$\n",
    "6. $B$ to $A$\n",
    "7. Why do the rows have to add up to 1, but the columns not?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write answers here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "revealable(ans509D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Intelligent Systems\n",
    "In Practice Problem B, the action taken did not depend on the state: $x$ was chosen or $y$ was chosen regardless of the current state.\n",
    "\n",
    "However, given that $A$ was the desired state, the most intelligent course of action would be the one most likely to return $A$, which was a different depending on whether the current state was $A$ or $B$.\n",
    "\n",
    "A system that monitors its own state and chooses the best action based on its state is said to display __intelligence__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem E\n",
    "Here are the matrices for actions x and y again:\n",
    "\n",
    "action x: $\\begin{array}{cc}\n",
    " \\\\\n",
    "\\begin{array}{cc}\n",
    "\\qquad \\color{green}{A} & \\color{green}{B}\n",
    "\\end{array} \\\\\n",
    "\\begin{array}{c}\n",
    "\\color{purple}{A} \\\\\n",
    "\\color{purple}{B}\n",
    "\\end{array}\n",
    "\\left[\\begin{array}{cc}\n",
    ".6 & .4 \\\\\n",
    ".3 & .7 \n",
    "\\end{array}\\right]\n",
    "\\end{array}$, action y: \n",
    "$\\begin{array}{cc}\n",
    " \\\\\n",
    "\\begin{array}{cc}\n",
    "\\qquad \\color{green}{A} & \\color{green}{B}\n",
    "\\end{array} \\\\\n",
    "\\begin{array}{c}\n",
    "\\color{purple}{A} \\\\\n",
    "\\color{purple}{B}\n",
    "\\end{array}\\\n",
    "\\left[\\begin{array}{cc}\n",
    ".5&.5 \\\\\n",
    ".8&.2\n",
    "\\end{array}\\right]\n",
    "\\end{array}$\n",
    "\n",
    "1. In state $A$, what action is the best?\n",
    "2. In state $B$, what action is the best?\n",
    "3. Write a program in Julia that chooses the best action based on the current state. Run several 100-time-step simulations using your intelligent program."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer #1 and #2 here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Program here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "revealable(ans509E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Applications of MDPs\n",
    "Most systems, of course, are far more complex than the two-state, two-action example given here. As you can imagine, running simulations for every possible action in every possible state requires some powerful computing. \n",
    "\n",
    "MDPs are used extensively in robotics, automated systems, manufacturing, and economics, among other fields.\n",
    "\n",
    "##POMDPs\n",
    "A variation on the traditional MDP is a __Partially Observable Markov Decision Process__ (POMDP, pronounced \"Pom D.P.\"}. In these scenarios, the system does not know exactly what state it is currently in, and therefore has to guess.\n",
    "\n",
    "This is like the difference between thinking, \n",
    "* \"I'm going in the right direction\"\n",
    "  and\n",
    "* \"I *think* I'm going in the right direction.\"\n",
    "\n",
    "POMDPs have the same elements as a traditional MDP, plus two more.\n",
    "The first is the belief state, which is the state the system believes it is in. The belief state is a probability distribution.\n",
    "For example, \n",
    "\"I think Iâ€™m going in the right direction\"\n",
    "might really mean:\n",
    "* 80% chance this is the right direction\n",
    "* 15% mostly-right direction\n",
    "* 5% completely wrong direction \n",
    "\n",
    "The second additional element is a set of __observations__. After the system takes an action based on its belief state, it observes what happens next and updates its belief state accordingly.\n",
    "\n",
    "For example, if you took a right turn and didnâ€™t see the freeway you expected, you would then change your probability distribution for whether you were going in the right direction.\n",
    "\n",
    "Otherwise, a POMDP works much like a traditional MDP:\n",
    "* An action is chosen based on the belief state *(not the current state, as in a traditional MDP)*\n",
    "* The next state is reached and the reward granted\n",
    "* An observation is made and the belief state updated *(This step is absent in a traditional MDP)*\n",
    "\n",
    "\n",
    "Then the next action is chosen based on the belief state and the process repeats.\n",
    "\n",
    "##Applications of POMDPs\n",
    "POMDPs are used in robotics, automated systems, medicine, machine maintenance, biology,  networks, linguistics, and many other fields.\n",
    "\n",
    "There are also many variations on POMDP frameworks depending on the application. Some rely on Monte Carlo-style simulations. Others are used in machine learning applications (in which computers learn from trial and error rather than relying only on programmersâ€™ instructions). \n",
    "Because POMDPs are widely applicable and also fairly new on the scene, fresh variations are in constant development.\n",
    "\n",
    "[More information is available online.](http://www.cc.gatech.edu/~mstilman/class/RIP13/materials/POMDPapplications.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem F\n",
    "Imagine that you are a doctor trying to help a sneezy patient.\n",
    "1. Name three possible states for \"Why this patient sneezes\" and assign a probability to each state. (This is your belief state.)\n",
    "2. Choose an action based on your belief state.\n",
    "3. Suppose the patient is still sneezing after your action. Update your belief state.\n",
    "4. Write a paragraph explaining your reasoning in 1 through 3, and what other steps you might take (including questions to ask or other actions to try) to refine your belief state."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Write your answer here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "revealable(ans509F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.10",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
