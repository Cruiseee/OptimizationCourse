{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 4-12 Gradient-Based Optimization, Part 2\n",
    "* Conjugate Gradient Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "using Revealables\n",
    "include(\"files/answers.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Premise\n",
    "<img src=\"files/4-12/convslow.png\" style=\"padding:0em 1em 0em 0em\" width=200 align=\"left\" />In the last lesson, we found that the steepest descent method is slow to converge when the gradient is close to 0. The reason is that each step is orthogonal to (at right angles to) the previous step.\n",
    "\n",
    "Part of the problem is that when the contour lines are non-circular, perpendicular (orthogonal) lines don't head exactly towards the center of the region.\n",
    "\n",
    "\n",
    "The __conjugate gradient method__ avoids the problems of the steepest descent method by nudging the vectors so that they are not precisely orthogonal to each other. \n",
    "\n",
    "<img src=\"files/4-12/nudge.png\" width=500 />\n",
    "\n",
    "Instead, they are distorted so that if the contours were circular, the vectors would be orthogonal; the less circular the contours, the more distortion is needed.\n",
    "\n",
    "Vectors formed by this type of distortion are called __conjugates__. \n",
    "\n",
    "There is a lot of complicated math behind how to accomplish this nudging of the vectors, which we won't get into. \n",
    "\n",
    "What you need to know is that it involves a multiplier, which can be found using the two vectors involved, in this formula:\n",
    "\n",
    "$$s=\\frac{v_n \\cdot v_n}{v_{n-1} \\cdot v_{n-1}}$$\n",
    "\n",
    "where the · symbol is the *dot product*, detailed in the next section. This formula is the same as finding the square of the magnitude of each vector, which is an alternate way of thinking about it.\n",
    "\n",
    "##Dot Products (Review)\n",
    "The dot product of two vectors a and b, symbolized a · b, is the sum of the products of corresponding elements. *(What?)*\n",
    "\n",
    "If $a = [a_1, a_2, a_3, ...]$ and $b = [b_1, b_2, b_3, ...]$,\n",
    "\n",
    "then $a · b = a_1b_1 + a_2b_2 + a_3b_3 + ...$.\n",
    "\n",
    "So, $$\\begin{align}\n",
    "\\left[ \\begin{array}{ccc} 3 & 1 & -2 \\end{array} \\right] · \\left[ \\begin{array}{ccc} -6 & 3 & 0\\end{array} \\right] &= \\\\\n",
    "\t-18 + 3 + 0 &= \\\\\n",
    "\t\t\t   &= -15\\end{align}$$\n",
    "\n",
    "In Julia, two column arrays' dot product can be found using `dot(a,b)`. The arrays have to be in *column form*, entered `[3, 1, -2]` or `[3; 1; -2]` (rather than `[3 1 -2]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem A\n",
    "Using the two given gradients, and the formula\n",
    "\n",
    "$$s=\\frac{v_n \\cdot v_n}{v_{n-1} \\cdot v_{n-1}}$$\n",
    "\n",
    "where v is the negative-gradient vector, find the value of the multiplier $s$.\n",
    "1. $g_0 = [3, 2]$, $g_1 = [0.5, -0.2]$\n",
    "2. $g_2 = [0.2, 0.01]$, $g_1 = [1, -0.2]$\n",
    "\n",
    "If you did these by hand, check your work using Julia. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Find the value of `s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "revealable(ans412A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#The First Steps\n",
    "The conjugate gradient method begins in exactly the same way as the steepest descent method:\n",
    "1. Find the gradient at the original point $x_0$ and use its negative vector as the steepest descent direction.\n",
    "2. Use $new~point = old point + scalar · vector$ to formulate a new point in terms of the scalar variable $a$. \n",
    "3. Substitute the $new~point$ into $f$ to obtain a single-variable function $f(a)$, then minimize $f(a)$.\n",
    "4. Substitute the minimized value of a into the $new~point$ formula to find the new point, $x_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem B\n",
    "Using the function $f(x, y) = (x – y)^2 + 20y^2$ at the initial point $(2, -2)$, use your steepest-descent algorithm or program from the last lesson to find the next point.\n",
    "\n",
    "Then, find the gradient and negative-gradient vector at the new point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "revealable(ans412B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Next Steps\n",
    "After the new point is found, its gradient and directional vector are calculated. The next step is to nudge this new vector, which is currently orthogonal to the previous one, so that it instead becomes the conjugate.\n",
    "\n",
    "First, calculate the multiplier:\n",
    "$$s=\\frac{v_n \\cdot v_n}{v_{n-1} \\cdot v_{n-1}}$$\n",
    "\n",
    "Then, form the new vector as follows:\n",
    "$$v = vn + sn·vn-1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem C\n",
    "From problem 2, you should have:\n",
    "* Original point $(2, -2)$ with gradient $[8  -88]$ and directional vector $<-8, 88>$\n",
    "* New point $(1.8097, 0.935)$ with gradient $[3.432, 0.308]$ and directional vector $<-3.432,  -0.308>$\n",
    "\n",
    "Use these to calculate the new (conjugate) vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Calculate here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "revealable(ans412C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Final Steps\n",
    "Once you have the conjugate vector, repeat the procedure with the current point and new vector.\n",
    "\n",
    "The steps, in order:\n",
    "1. From point $x_0$:\n",
    "  * use gradient to get vector $v_0$\n",
    "  * minimize $f(x_0 + a·v_0)$ to get $x_1$.<br /><br />\n",
    "2. At point $x_1$:\n",
    "  * use gradient to get vector $v_1$\n",
    "  * calculate $s$\n",
    "  * nudge vector $v_1$ by $s·v_0$ to get modified $v_1$\n",
    "  * minimize $f(x_1 + a·v_1)$ to get $x_2$.<br /><br />\n",
    "3. Repeat Step 2 until convergence is attained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem D\n",
    "Using the conjugate vector $<-3.4452, -0.174>$ and the point $(1.8097, 0.0935)$ from Problem C, complete one more iteration of the conjugate gradient  method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# One more time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "revealable(ans412D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem E\n",
    "Modify your steepest descent program to include the conjugate-gradient vector nudge.\n",
    "\n",
    "Then, use it to find the minimum of \n",
    "\t$f(x, y) = (x – 2)^4 + (x – 2y)^2$\n",
    "from initial point $(0, 0)$ – the same problem that caused trouble with steepest-descent in the last lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Edit your steepest descent program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Test your program here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "revealable(ans412E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.10",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
