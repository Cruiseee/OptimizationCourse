{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# 4-11 Gradient-Based Optimization, Part 1\n",
    "* Gradients in Calculus.jl\n",
    "* Steepest Descent Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using Revealables\n",
    "include(\"files/answers.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using the `Calculus` Package\n",
    "The language for finding gradients and Hessians in the Calculus package for Julia is a little complex. Try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Calculus\n",
    "f(x, y) = x^2 + 2x*y\n",
    "g = Calculus.gradient(x -> f(x[1], x[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a function `g` (you could name it whatever you like) that finds the gradient of `f` at a point. \n",
    "You can evaluate the gradient using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g([2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give you `[6.0; 4.0]`.\n",
    "\n",
    "Verify that this is, in fact, the gradient of $x^2 + 2xy$ at $(2, 1)$.\n",
    "\n",
    "You can also evaluate the Hessian using similar language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x, y) = x^2 + 2x*y\n",
    "h = hessian(x -> f(x[1], x[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h([2.0, 1.0])  # This should give the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eig(h([2.0, 1.0]))  # Should give -1.236, 3.236 -- a saddle point, apparently!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Tips When Using `Calculus`\n",
    "* Remember to start with `using Calculus` every time you open a new interface with Julia.\n",
    "* Note that `x[1]`, `x[2]` and `x[3]` must be used within `gradient` and `hessian` instead of `x`, `y`, `z`. This indexes the elements in the array x.\n",
    "* When in doubt, type in decimals. This creates a `float64` array instead an integer (`int64`) array, which will prevent most type-based errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem A\n",
    "Use the Calculus package to evaluate the gradient and the eigenvalues of the Hessian of:\n",
    "1. $5x^2 – 3xy^2 + 2y$ at $(1, -5)$\n",
    "2. $3xy + 2x^2z – y^2z$ at $(3, 1, 0)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revealable(ans411A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Minimizing With Steepest Descent\n",
    "The Steepest Descent Method for minimization begins with the idea that the gradient is a vector pointing in the direction of fastest increase for the function. The steepest descent method uses the <font color=\"red\">negative</font> value of the gradient to find the ideal direction of decrease. \n",
    "\n",
    "After that, it works much like the Cyclic Coordinate Search technique, where the function is minimized in that direction and the procedure repeated.\n",
    "\n",
    "##First Steps\n",
    "Suppose you are trying to minimize $f(x, y) = x^2 – 4x + 5y^2 – 3y$,\n",
    "from initial point $(5, 6)$.\n",
    "\n",
    "First, you would find the gradient at $(5, 6)$. This comes out to `[6  57]`, which gives us the direction of <font color=\"blue\">ascent</font>. Since we are trying to <font color=\"red\">minimize</font>, we will use the opposite vector: $<-6, -57>$.\n",
    "\n",
    "The next step is to use the vector translation formula: \n",
    "\t$$new~point = old~point + scalar · vector$$\n",
    "to get coordinates for a new point:\n",
    "\t$$\\begin{align}\n",
    "    new~point &= (5, 6) + a · <-6, -57> \\\\\n",
    "\tnew~point &= (5 – 6a, 6 – 57a)\\end{align}$$\n",
    "    \n",
    "Then we can plug this new point back into the function: $f(\\color{green}{x}, \\color{purple}{y}) = \\color{green}{x}^2 – 4\\color{green}{x} + 5\\color{purple}{y}^2 – 3\\color{purple}{y}$ becomes \n",
    "$$f(a) = \\color{green}{(5 – 6a)}^2 – 4\\color{green}{(5 – 6a)} + 5\\color{purple}{(6 – 57a)}^2 – 3\\color{purple}{(6 – 57a)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem B\n",
    "Let $f(x, y) = x^2 + 2y^2 – y$. \n",
    "1. Find the gradient at the initial point $(10, 12)$.\n",
    "2. Use the negative gradient to transform $f(x, y)$ to a function $f(a)$ using a for the unknown scalar multiplier. Do not simplify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revealable(ans411B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Next Steps\n",
    "After transforming $f(x, y)$ into $f(a)$, the next step is to minimize $f(a)$.\n",
    "\n",
    "You can then substitute the minimized a value into the equations for $x$ and $y$ to find the actual coordinates of the new point. \n",
    "\n",
    "Essentially, what this procedure does is find the vector of steepest descent, then create the cross-section along that vector, then minimize the function along the cross-section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem C\n",
    "1. Using any minimization program from Unit 2, minimize your $f(a)$ from Problem A. \n",
    "2. Use the resulting value of $a$ to find the new point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run an old program here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the new point here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revealable(ans411C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##The Last Step\n",
    "The last step is to repeat the procedure until a condition is met – often one or a combination of: \n",
    "* a number of iterations\n",
    "* a low enough value of the gradient (which approaches 0 as the optimal point is neared)\n",
    "* a low enough change in the vector $a$\n",
    "* a low enough change in the function value between the old and new points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem D\n",
    "Repeat the procedure twice more for the function $f(x, y) = x^2 + 2y^2 – y$.\n",
    "\n",
    "Automate as much as possible, but don’t write a full program (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the new point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revealable(ans411D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem E\n",
    "Write a program that will repeat the procedure for $n$ iterations. \n",
    "\n",
    "The program should print out the last two calculated points (so you can see if you need to raise the iterations for better convergence).\n",
    "\n",
    "Test for 5 iterations, then 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revealable(ans411E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Practice Problem F\n",
    "Use your program to minimize \n",
    "$f(x, y) = (x – 2)^4 + (x – 2y)^2$\n",
    "from initial point $(0, 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run your program here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "revealable(ans411F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Problems with Steepest Descent\n",
    "As you may have noticed on Problem E, there are occasions where the function may take a very long time to converge using the steepest descent method. \n",
    "\n",
    "The reason is that if the function is very flat near the minimum (as in an $x^4$ function), the gradient gets very small and each iteration goes a very short distance.\n",
    "\n",
    "There are refinements, but the best one is an entirely different method – next lesson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
